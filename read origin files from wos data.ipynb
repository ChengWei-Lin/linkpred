{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read from wos data then transform to node and edge csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "scient_coauthor_path = path+'\\\\data\\\\scientometrics\\\\co-author\\\\'\n",
    "scient_cooccurr_path = path+'\\\\data\\\\scientometrics\\\\co-occurrence\\\\'\n",
    "jasist_coauthor_path = path+'\\\\data\\\\jasist\\\\co-author\\\\'\n",
    "jasist_cooccurr_path = path+'\\\\data\\\\jasist\\\\co-occurrence\\\\'\n",
    "\n",
    "output_scient_coauthor_path = path+\"\\\\data transformation output\\\\scientometrics\\\\co-author\\\\\"\n",
    "output_scient_cooccurr_path = path+\"\\\\data transformation output\\\\scientometrics\\\\co-occurrence\\\\\"\n",
    "output_jasist_coauthor_path = path+\"\\\\data transformation output\\\\jasist\\\\co-author\\\\\"\n",
    "output_jasist_cooccurr_path = path+\"\\\\data transformation output\\\\jasist\\\\co-occurrence\\\\\"\n",
    "\n",
    "f_paths = [#scient_coauthor_path\\\n",
    "           scient_cooccurr_path\\\n",
    "           #, jasist_coauthor_path\\\n",
    "           , jasist_cooccurr_path\\\n",
    "          ]\n",
    "output_f_paths = [#output_scient_coauthor_path\\\n",
    "                  output_scient_cooccurr_path\\\n",
    "                  #, output_jasist_coauthor_path\\\n",
    "                  , output_jasist_cooccurr_path\\\n",
    "                 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_wos_files_paths(files_path):\n",
    "    files = os.listdir(files_path)\n",
    "    wos_txt = []\n",
    "    for file in files:\n",
    "        fs = os.listdir(files_path+file)\n",
    "        for f in fs:\n",
    "#             if f.endswith(').txt'):\n",
    "                wos_txt.append(files_path+file+\"\\\\\"+f)\n",
    "    return files, wos_txt\n",
    "\n",
    "def read_wosfile_to_df(times, wos_filepaths):\n",
    "    df_all_time = []\n",
    "    for t in times:\n",
    "        df = pd.DataFrame()\n",
    "        for i, file in enumerate(wos_filepaths):\n",
    "            if t in file:\n",
    "                # print(i, t)\n",
    "                df_iter = pd.read_csv(file, sep='\\t', index_col=False)\n",
    "                df = df.append(df_iter)\n",
    "        df_all_time.append(df)\n",
    "    return df_all_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save wos txt files to csv in data transformation output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def main(f_path):\n",
    "#     times, wos_filepaths = list_wos_files_paths(f_path)\n",
    "#     df_all_times = read_wosfile_to_df(times, wos_filepaths)\n",
    "    \n",
    "#     file_pref = '_'.join(f_path.split('\\\\')[-3:-1])\n",
    "#     for i, df in enumerate(df_all_times):\n",
    "#         df.to_csv(output_path+file_pref+\"_{}.csv\".format(times[i]))\n",
    "#         print(\"file saved:\\t {}\".format(file_pref+\"_{}.csv\".format(times[i])))\n",
    "    \n",
    "# for f_path in f_paths:\n",
    "#     print(\"-----{}------\".format(f_path))\n",
    "#     main(f_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for f_i, f_path in enumerate(f_paths):\n",
    "#     print('\\n', \"-\"*10, f_path)\n",
    "#     times, wos_filepaths = list_wos_files_paths(f_path)\n",
    "#     df_all_times = read_wosfile_to_df(times, wos_filepaths)\n",
    "        \n",
    "#     for i, df in enumerate(df_all_times):\n",
    "#         df.to_csv(output_f_paths[f_i]+\"{}_wos.csv\".format(times[i]), index=None)\n",
    "#         print(\"file saved:\\t {}\".format(output_f_paths[f_i]+\"{}_wos.csv\".format(times[i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate keywords table with id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_kw(df_all_time):\n",
    "    afs = list(df_all_time['DE'])\n",
    "    new_afs = []\n",
    "    for af in afs:\n",
    "        if type(af)!=float:\n",
    "            new_afs.append(af.split('; '))\n",
    "        else:\n",
    "            new_afs.append([])\n",
    "    # flatten list\n",
    "    all_afs = [name for names in new_afs for name in names]\n",
    "    \n",
    "    return new_afs, sorted(list(set(all_afs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate author full name table with id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_af(df_all_time):\n",
    "    aus = list(df_all_time['AU'])\n",
    "    afs = list(df_all_time['AF'])\n",
    "\n",
    "    new_aus, new_afs = [], []\n",
    "\n",
    "    for au in aus:\n",
    "        new_aus.append(au.split('; '))\n",
    "    for af in afs:\n",
    "        new_afs.append(af.split('; '))\n",
    "    \n",
    "    # flatten list\n",
    "    all_aus = [name for names in new_aus for name in names]\n",
    "    all_afs = [name for names in new_afs for name in names]\n",
    "\n",
    "    print(\"List of origin all names:\\t {} AU; \\t {} AF\".format(len(all_aus), len(all_afs)))\n",
    "    print(\"Set  of origin all names:\\t {} AU; \\t {} AF\".format(len(list(set(all_aus))), len(list(set(all_afs))))) # full name has more values than abr. name\n",
    "    \n",
    "    return new_afs, sorted(list(set(all_afs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all authors in all times and set it\n",
    "def generate_all_names(all_afs_set):\n",
    "    all_names = []\n",
    "    for afs_set in all_afs_set:\n",
    "        all_names+=afs_set\n",
    "    all_names = sorted(list(set(all_names)))\n",
    "    \n",
    "    # all_nodes_table\n",
    "    df_all_names = pd.DataFrame(columns=['id', 'label'])\n",
    "    df_all_names['label'] = all_names\n",
    "    df_all_names['id'] = list(range(1, len(all_names)+1))\n",
    "\n",
    "    return all_names, df_all_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generate_all_id_cooccurs(all_afs, all_names):\n",
    "    all_id_cooccurs = []\n",
    "\n",
    "    for afs in all_afs:\n",
    "        id_cooccurs_list = []\n",
    "        for cooccurs in afs:\n",
    "            id_cooccurs = []\n",
    "            for name in cooccurs:\n",
    "                id_cooccurs.append(all_names.index(name)+1)\n",
    "            id_cooccurs_list.append(id_cooccurs)\n",
    "        all_id_cooccurs.append(id_cooccurs_list)\n",
    "        \n",
    "    return all_id_cooccurs\n",
    "\n",
    "def generate_all_pair_lists(all_id_cooccurs):\n",
    "    all_pair_lists = []\n",
    "    for cooccurence_list in all_id_cooccurs:\n",
    "        all_pair_list = []\n",
    "        for cooccur in cooccurence_list:\n",
    "            pair_list = []\n",
    "            for pair in itertools.combinations(cooccur, 2):\n",
    "                pair_list.append(pair)\n",
    "\n",
    "            all_pair_list+=pair_list\n",
    "\n",
    "        all_pair_lists.append(all_pair_list)\n",
    "    return all_pair_lists\n",
    "        \n",
    "        \n",
    "def generate_df_edges(all_pair_lists):    \n",
    "    df_edges = []\n",
    "    for all_pair_list in all_pair_lists:\n",
    "        print(\"pair length:\\t\", len(all_pair_list))\n",
    "\n",
    "        df_edge = pd.DataFrame(columns=['source', 'target', 'weight'])\n",
    "        source_list, target_list = [], []\n",
    "\n",
    "        for pair in all_pair_list:\n",
    "            source_list.append(pair[0])\n",
    "            target_list.append(pair[1])\n",
    "\n",
    "        df_edge['source'] = source_list\n",
    "        df_edge['target'] = target_list\n",
    "        df_edge['weight'] = [1]*len(all_pair_list)\n",
    "        df_edge = df_edge.sort_values(by=['source', 'target'])\n",
    "        df_edge.index = list(range(len(df_edge)))\n",
    "\n",
    "        df_edges.append(df_edge)\n",
    "\n",
    "    return df_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### print drop duplicates should be the same length as after generate_weight function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_weight(df):\n",
    "    # x = pd.DataFrame(columns=['source', 'target', 'weight'])\n",
    "    # x['source'] = [1,1,1,2,3,3,4,5,5,5]\n",
    "    # x['target'] = [2,2,2,3,4,4,5,1,1,1]\n",
    "    # x['weight'] = [1,1,1,1,1,1,1,1,1,1]\n",
    "    # x = df_edges[0][['source', 'target', 'weight']]\n",
    "    \n",
    "    x = df[['source', 'target', 'weight']]\n",
    "    x.loc[len(x.index)] = ['end', 'end', 1] # use end index to subtract the weight by index number in the end\n",
    "\n",
    "    y = x.drop_duplicates(subset=['source', 'target'])\n",
    "    y_index = list(y.index)\n",
    "    # print(y_index)\n",
    "    new_weight = []\n",
    "    for i, index in enumerate(y_index):\n",
    "        if i+1<len(y_index):\n",
    "            new_weight.append(y_index[i+1]-y_index[i])\n",
    "    y = y[y['source']!='end']\n",
    "    y['weight'] = new_weight\n",
    "    y = y.sort_values(by=['source', 'target'])\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main for author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_au(f_path):\n",
    "    times, wos_filepaths = list_wos_files_paths(f_path)\n",
    "    df_all_times = read_wosfile_to_df(times, wos_filepaths)\n",
    "\n",
    "    all_afs, all_afs_set = [], []\n",
    "    for df_all_time in df_all_times:\n",
    "        afs, afs_set = generate_af(df_all_time)\n",
    "        all_afs.append(afs)\n",
    "        all_afs_set.append(afs_set)\n",
    "        \n",
    "    all_names = [name for names in all_afs_set for name in names]\n",
    "    all_names = list(set(all_names))\n",
    "    print(\"\\n Nodes of all time: \\t\", len(all_names))\n",
    "\n",
    "    df_nodes = []\n",
    "    for nodes in all_afs_set:\n",
    "        df_node = pd.DataFrame(columns=['id', 'label'])\n",
    "\n",
    "        node_id_list, node_label_list = [], []\n",
    "        for name in nodes:\n",
    "            node_id_list.append(all_names.index(name)+1)\n",
    "            node_label_list.append(name)\n",
    "\n",
    "        df_node['id'] = node_id_list\n",
    "        df_node['label'] = node_label_list\n",
    "        df_nodes.append(df_node)\n",
    "\n",
    "    all_id_cooccurs = generate_all_id_cooccurs(all_afs, all_names)\n",
    "    all_pair_lists = generate_all_pair_lists(all_id_cooccurs)\n",
    "    print()\n",
    "    df_edges = generate_df_edges(all_pair_lists)\n",
    "    \n",
    "    print(\"\\nUnique edge before generate weight: \\n \")\n",
    "    for i, df in enumerate(df_edges):\n",
    "        print(times[i], \"\\t\", len(df.drop_duplicates(subset=['source', 'target'])), 'edges')\n",
    "\n",
    "    print(\"\\nUnique edge after generate weight: \\n \")\n",
    "    df_weight_edges = []\n",
    "    for i, df_edge in enumerate(df_edges):\n",
    "        df_iter = generate_weight(df_edge)\n",
    "        df_weight_edges.append(df_iter)\n",
    "        print(times[i], \"\\t\", len(df_iter), 'edges')\n",
    "        \n",
    "    return times, df_all_times, df_nodes, df_weight_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main for keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_kw(f_path):\n",
    "    times, wos_filepaths = list_wos_files_paths(f_path)\n",
    "    df_all_times = read_wosfile_to_df(times, wos_filepaths)\n",
    "\n",
    "    all_afs, all_afs_set = [], []\n",
    "    for df_all_time in df_all_times:\n",
    "        afs, afs_set = generate_kw(df_all_time)\n",
    "        all_afs.append(afs)\n",
    "        all_afs_set.append(afs_set)\n",
    "        \n",
    "    all_names = [name for names in all_afs_set for name in names]\n",
    "    all_names = list(set(all_names))\n",
    "    print(\"\\n Nodes of all time: \\t\", len(all_names))\n",
    "\n",
    "    df_nodes = []\n",
    "    for nodes in all_afs_set:\n",
    "        df_node = pd.DataFrame(columns=['id', 'label'])\n",
    "\n",
    "        node_id_list, node_label_list = [], []\n",
    "        for name in nodes:\n",
    "            node_id_list.append(all_names.index(name)+1)\n",
    "            node_label_list.append(name)\n",
    "\n",
    "        df_node['id'] = node_id_list\n",
    "        df_node['label'] = node_label_list\n",
    "        df_nodes.append(df_node)\n",
    "\n",
    "    all_id_cooccurs = generate_all_id_cooccurs(all_afs, all_names)\n",
    "    all_pair_lists = generate_all_pair_lists(all_id_cooccurs)\n",
    "    print()\n",
    "    df_edges = generate_df_edges(all_pair_lists)\n",
    "    \n",
    "    print(\"\\nUnique edge before generate weight: \\n \")\n",
    "    for i, df in enumerate(df_edges):\n",
    "        print(times[i], \"\\t\", len(df.drop_duplicates(subset=['source', 'target'])), 'edges')\n",
    "\n",
    "    print(\"\\nUnique edge after generate weight: \\n \")\n",
    "    df_weight_edges = []\n",
    "    for i, df_edge in enumerate(df_edges):\n",
    "        df_iter = generate_weight(df_edge)\n",
    "        df_weight_edges.append(df_iter)\n",
    "        print(times[i], \"\\t\", len(df_iter), 'edges')\n",
    "        \n",
    "    return times, df_all_times, df_nodes, df_weight_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "f_paths = [path+\"\\\\data\\\\scientometrics_2010-2019\\\\\"]\n",
    "output_f_paths = [path+\"\\\\data transformation output\\\\scientometrics\\\\co-author\\\\\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Nodes of all time: \t 7512\n",
      "\n",
      "pair length:\t 18083\n",
      "pair length:\t 3887\n",
      "pair length:\t 8135\n",
      "pair length:\t 12634\n",
      "pair length:\t 16566\n",
      "\n",
      "Unique edge before generate weight: \n",
      " \n",
      "2010-2015 \t 17029 edges\n",
      "2016-2016 \t 3811 edges\n",
      "2016-2017 \t 7921 edges\n",
      "2016-2018 \t 12232 edges\n",
      "2016-2019 \t 15957 edges\n",
      "\n",
      "Unique edge after generate weight: \n",
      " \n",
      "2010-2015 \t 17029 edges\n",
      "2016-2016 \t 3811 edges\n",
      "2016-2017 \t 7921 edges\n",
      "2016-2018 \t 12232 edges\n",
      "2016-2019 \t 15957 edges\n"
     ]
    }
   ],
   "source": [
    "times, df_all_times, df_nodes, df_weight_edges = main_kw(f_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = df_all_times[0]['DE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_time = df_all_times[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1, y2 = generate_kw(df_all_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Women; Networks; Advice; Support; Field of science'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr.iloc[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Journal impact factor; Impact factor; Journal citation reports; Citation analysis; Scientific journals; Citation indicators; Bibliometrics; Influence measures; Criticism; Research evaluation\n"
     ]
    }
   ],
   "source": [
    "for string in arr:\n",
    "    if type(string)!=float:\n",
    "        \n",
    "        if \"'Journal impact factor\" in string:\n",
    "            print(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "260\n",
      "260 \t ['Women scientists', 'Russia', 'Bibliometrics', 'Surnames', 'Citations']\n",
      "261 \t ['Composite indicators', 'Rankings', 'Benchmarking', 'Higher Education Institutions', 'Weighting schemes', 'Simulation techniques']\n",
      "262 \t ['Durability', 'Obsolescence', 'Bibliometric indicators', 'Individual level analysis', 'Micro-level analysis', 'Mendel syndrome']\n",
      "263 \t ['Research productivity', 'Academic psychologists', 'Measurement']\n",
      "264 \t ['Publication analysis', 'First quartile journals', 'Overall prestige', 'Ranking methods', 'Axiomatic index', 'Longitudinal analysis']\n",
      "265 \t ['Citation analysis', 'Matthew effect', 'Scholarly communication']\n",
      "266 \t ['Patent citation', 'Electronic-paper display technology', 'Emerging field']\n",
      "267 \t ['Bibliometrics', 'Biomedical journals', 'Impact factor', 'Nobel prize', 'Publication productivity', 'Surnames']\n",
      "268 \t ['Astronomical journals', 'Publications', 'Citations']\n",
      "269 \t ['Citations', 'Co-authors', 'Pareto distribution']\n",
      "270 \t ['Bibliometric indicator', 'Citation impact', 'Field normalization', 'Recursive indicator']\n",
      "271 \t ['Bibliometrics', 'Thermodynamics', 'Exergy', 'Energy', 'Entropy', 'S = E - X', 'p-Index', 'Composite indicators']\n",
      "272 \t ['Impact factor', 'Citation', 'Citation normalization', 'Citing-side normalization', 'Source-level normalization']\n",
      "273 \t ['Retrieval system', 'Value-added services', 'Science models', 'IR', 'Re-ranking', 'Evaluation']\n",
      "274 \t ['Multiple authorship', 'Fractional allocations', 'h-Index']\n",
      "275 \t ['Medline', 'Co-term', 'Health', 'Women', 'Research', 'Social networks', 'PFNET', 'Maps']\n",
      "276 \t ['Agriculture', 'Intramural research', 'Research benchmarking', 'Research output', 'USDA', 'Federal research', 'Education', 'Extension']\n",
      "277 \t ['h-index', 'Citation analysis', 'Bibliometric indexes', 'Research career evaluation']\n",
      "278 \t ['Scientometrics', 'Incentives', 'Research policy', 'Quasi experimental design', 'South Africa']\n",
      "279 \t ['Rankings', 'Universities', 'Higher education', 'Bibliometrics', 'Shanghai ranking', 'Bidimensional analysis', 'Evaluation models', 'Research performance assessment', 'h-index']\n"
     ]
    }
   ],
   "source": [
    "s = int(input())\n",
    "e = s+20\n",
    "for i in range(s, e):\n",
    "    print(i, '\\t', y1[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_kw(df_all_time):\n",
    "    afs = list(df_all_time['DE'])\n",
    "    new_afs = []\n",
    "    for af in afs:\n",
    "        if type(af)!=float:\n",
    "            new_afs.append(af.split('; '))\n",
    "        else:\n",
    "            new_afs.append([])\n",
    "    # flatten list\n",
    "    all_afs = [name for names in new_afs for name in names]\n",
    "    \n",
    "    return new_afs, sorted(list(set(all_afs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ---------- C:\\Users\\Liser\\Desktop\\linchengwei_link_prediction\\data\\scientometrics_2010-2019\\\n",
      "List of origin all names:\t 4375 AU; \t 4375 AF\n",
      "Set  of origin all names:\t 2744 AU; \t 2900 AF\n",
      "List of origin all names:\t 980 AU; \t 980 AF\n",
      "Set  of origin all names:\t 861 AU; \t 863 AF\n",
      "List of origin all names:\t 2056 AU; \t 2056 AF\n",
      "Set  of origin all names:\t 1644 AU; \t 1668 AF\n",
      "List of origin all names:\t 3036 AU; \t 3036 AF\n",
      "Set  of origin all names:\t 2258 AU; \t 2305 AF\n",
      "List of origin all names:\t 3916 AU; \t 3916 AF\n",
      "Set  of origin all names:\t 2814 AU; \t 2876 AF\n",
      "\n",
      " Nodes of all time: \t 5190\n",
      "\n",
      "pair length:\t 5478\n",
      "pair length:\t 1289\n",
      "pair length:\t 2733\n",
      "pair length:\t 4014\n",
      "pair length:\t 5258\n",
      "\n",
      "Unique edge before generate weight: \n",
      " \n",
      "2010-2015 \t 4865 edges\n",
      "2016-2016 \t 1248 edges\n",
      "2016-2017 \t 2584 edges\n",
      "2016-2018 \t 3721 edges\n",
      "2016-2019 \t 4813 edges\n",
      "\n",
      "Unique edge after generate weight: \n",
      " \n",
      "2010-2015 \t 4865 edges\n",
      "2016-2016 \t 1248 edges\n",
      "2016-2017 \t 2584 edges\n",
      "2016-2018 \t 3721 edges\n",
      "2016-2019 \t 4813 edges\n"
     ]
    }
   ],
   "source": [
    "for f_i, f_path in enumerate(f_paths):\n",
    "    print('\\n', \"-\"*10, f_path)\n",
    "    times, df_all_times, df_nodes, df_weight_edges = main_au(f_path)\n",
    "    \n",
    "    for i, df in enumerate(df_all_times):\n",
    "        df.to_csv(output_f_paths[f_i]+\"{}_wos.csv\".format(times[i]), index=None)\n",
    "    \n",
    "    for i, df_node in enumerate(df_nodes):\n",
    "        df_node.to_csv(output_f_paths[f_i]+\"{}_node.csv\".format(times[i]), index=None)\n",
    "    \n",
    "    for i, df_edge in enumerate(df_weight_edges):\n",
    "        df_edge.to_csv(output_f_paths[f_i]+\"{}_edge.csv\".format(times[i]), index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ---------- C:\\Users\\Liser\\Desktop\\linchengwei_link_prediction\\data\\scientometrics\\co-occurrence\\\n",
      "\n",
      " Nodes of all time: \t 7692\n",
      "\n",
      "pair length:\t 18460\n",
      "pair length:\t 22504\n",
      "pair length:\t 26791\n",
      "pair length:\t 31457\n",
      "pair length:\t 35745\n",
      "\n",
      "Unique edge before generate weight: \n",
      " \n",
      "2010-2015 \t 17305 edges\n",
      "2010-2016 \t 21010 edges\n",
      "2010-2017 \t 25032 edges\n",
      "2010-2018 \t 29350 edges\n",
      "2010-2019 \t 33293 edges\n",
      "\n",
      "Unique edge after generate weight: \n",
      " \n",
      "2010-2015 \t 17305 edges\n",
      "2010-2016 \t 21010 edges\n",
      "2010-2017 \t 25032 edges\n",
      "2010-2018 \t 29350 edges\n",
      "2010-2019 \t 33293 edges\n",
      "\n",
      " ---------- C:\\Users\\Liser\\Desktop\\linchengwei_link_prediction\\data\\jasist\\co-occurrence\\\n",
      "\n",
      " Nodes of all time: \t 386\n",
      "\n",
      "pair length:\t 714\n",
      "pair length:\t 1124\n",
      "pair length:\t 1124\n",
      "pair length:\t 1124\n",
      "pair length:\t 1124\n",
      "\n",
      "Unique edge before generate weight: \n",
      " \n",
      "2010-2015 \t 650 edges\n",
      "2010-2016 \t 1018 edges\n",
      "2010-2017 \t 1018 edges\n",
      "2010-2018 \t 1018 edges\n",
      "2010-2019 \t 1018 edges\n",
      "\n",
      "Unique edge after generate weight: \n",
      " \n",
      "2010-2015 \t 650 edges\n",
      "2010-2016 \t 1018 edges\n",
      "2010-2017 \t 1018 edges\n",
      "2010-2018 \t 1018 edges\n",
      "2010-2019 \t 1018 edges\n"
     ]
    }
   ],
   "source": [
    "for f_i, f_path in enumerate(f_paths):\n",
    "    print('\\n', \"-\"*10, f_path)\n",
    "    times, df_all_times, df_nodes, df_weight_edges = main(f_path)\n",
    "    \n",
    "    for i, df in enumerate(df_all_times):\n",
    "        df.to_csv(output_f_paths[f_i]+\"{}_wos.csv\".format(times[i]), index=None)\n",
    "    \n",
    "    for i, df_node in enumerate(df_nodes):\n",
    "        df_node.to_csv(output_f_paths[f_i]+\"{}_node.csv\".format(times[i]), index=None)\n",
    "    \n",
    "    for i, df_edge in enumerate(df_weight_edges):\n",
    "        df_edge.to_csv(output_f_paths[f_i]+\"{}_edge.csv\".format(times[i]), index=None)\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
