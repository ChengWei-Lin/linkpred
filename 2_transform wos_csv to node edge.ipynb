{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\USER\\\\Desktop\\\\linchengwei_linkpred'"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scientometrics / jasist: scientometrics\n",
      "co-author / co-occurrence: co-occurrence\n"
     ]
    }
   ],
   "source": [
    "journal = input(\"scientometrics / jasist: \")\n",
    "entity = input(\"co-author / co-occurrence: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\USER\\\\Desktop\\\\linchengwei_linkpred\\\\data\\\\scientometrics\\\\'"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = path+\"\\\\data\\\\{}\\\\\".format(journal)\n",
    "data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2010-2015', '2016-2016', '2016-2017', '2016-2018', '2016-2019']"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "times = os.listdir(data_path)\n",
    "times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\USER\\\\Desktop\\\\linchengwei_linkpred\\\\data\\\\scientometrics\\\\2010-2015',\n",
       " 'C:\\\\Users\\\\USER\\\\Desktop\\\\linchengwei_linkpred\\\\data\\\\scientometrics\\\\2016-2016',\n",
       " 'C:\\\\Users\\\\USER\\\\Desktop\\\\linchengwei_linkpred\\\\data\\\\scientometrics\\\\2016-2017',\n",
       " 'C:\\\\Users\\\\USER\\\\Desktop\\\\linchengwei_linkpred\\\\data\\\\scientometrics\\\\2016-2018',\n",
       " 'C:\\\\Users\\\\USER\\\\Desktop\\\\linchengwei_linkpred\\\\data\\\\scientometrics\\\\2016-2019']"
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_time_paths = [data_path+t for t in times]\n",
    "data_time_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read data from data_transformation_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- 2010-2015\n",
      "---------- 2016-2016\n",
      "---------- 2016-2017\n",
      "---------- 2016-2018\n",
      "---------- 2016-2019\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\USER\\\\Desktop\\\\linchengwei_linkpred\\\\data_transformation_output\\\\scientometrics\\\\wos_csv\\\\2010-2015.csv',\n",
       " 'C:\\\\Users\\\\USER\\\\Desktop\\\\linchengwei_linkpred\\\\data_transformation_output\\\\scientometrics\\\\wos_csv\\\\2016-2016.csv',\n",
       " 'C:\\\\Users\\\\USER\\\\Desktop\\\\linchengwei_linkpred\\\\data_transformation_output\\\\scientometrics\\\\wos_csv\\\\2016-2017.csv',\n",
       " 'C:\\\\Users\\\\USER\\\\Desktop\\\\linchengwei_linkpred\\\\data_transformation_output\\\\scientometrics\\\\wos_csv\\\\2016-2018.csv',\n",
       " 'C:\\\\Users\\\\USER\\\\Desktop\\\\linchengwei_linkpred\\\\data_transformation_output\\\\scientometrics\\\\wos_csv\\\\2016-2019.csv']"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = path+\"\\\\data_transformation_output\\\\{}\\\\wos_csv\\\\\".format(journal)\n",
    "data_path_origin = [data_path+f for f in os.listdir(data_path) if f.endswith('.csv')]\n",
    "\n",
    "data = dict()\n",
    "for i, time in enumerate(times):\n",
    "    print('-'*10, time)\n",
    "    data[time] = pd.read_csv(data_path_origin[i], index_col=0)\n",
    "    \n",
    "data_path_origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- 2010-2015\n",
      "---------- 2016-2016\n",
      "---------- 2016-2017\n",
      "---------- 2016-2018\n",
      "---------- 2016-2019\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\USER\\\\Desktop\\\\linchengwei_linkpred\\\\data_transformation_output\\\\scientometrics\\\\wos_csv\\\\stem\\\\2010-2015.csv',\n",
       " 'C:\\\\Users\\\\USER\\\\Desktop\\\\linchengwei_linkpred\\\\data_transformation_output\\\\scientometrics\\\\wos_csv\\\\stem\\\\2016-2016.csv',\n",
       " 'C:\\\\Users\\\\USER\\\\Desktop\\\\linchengwei_linkpred\\\\data_transformation_output\\\\scientometrics\\\\wos_csv\\\\stem\\\\2016-2017.csv',\n",
       " 'C:\\\\Users\\\\USER\\\\Desktop\\\\linchengwei_linkpred\\\\data_transformation_output\\\\scientometrics\\\\wos_csv\\\\stem\\\\2016-2018.csv',\n",
       " 'C:\\\\Users\\\\USER\\\\Desktop\\\\linchengwei_linkpred\\\\data_transformation_output\\\\scientometrics\\\\wos_csv\\\\stem\\\\2016-2019.csv']"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path_stem = [data_path+\"stem\\\\\"+f for f in os.listdir(data_path+\"stem\")]\n",
    "\n",
    "data_stem = dict()\n",
    "for i, time in enumerate(times):\n",
    "    print('-'*10, time)\n",
    "    data_stem[time] = pd.read_csv(data_path_stem[i], index_col=0)\n",
    "    \n",
    "data_path_stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- 2010-2015\n",
      "---------- 2016-2016\n",
      "---------- 2016-2017\n",
      "---------- 2016-2018\n",
      "---------- 2016-2019\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\USER\\\\Desktop\\\\linchengwei_linkpred\\\\data_transformation_output\\\\scientometrics\\\\wos_csv\\\\lemma\\\\2010-2015.csv',\n",
       " 'C:\\\\Users\\\\USER\\\\Desktop\\\\linchengwei_linkpred\\\\data_transformation_output\\\\scientometrics\\\\wos_csv\\\\lemma\\\\2016-2016.csv',\n",
       " 'C:\\\\Users\\\\USER\\\\Desktop\\\\linchengwei_linkpred\\\\data_transformation_output\\\\scientometrics\\\\wos_csv\\\\lemma\\\\2016-2017.csv',\n",
       " 'C:\\\\Users\\\\USER\\\\Desktop\\\\linchengwei_linkpred\\\\data_transformation_output\\\\scientometrics\\\\wos_csv\\\\lemma\\\\2016-2018.csv',\n",
       " 'C:\\\\Users\\\\USER\\\\Desktop\\\\linchengwei_linkpred\\\\data_transformation_output\\\\scientometrics\\\\wos_csv\\\\lemma\\\\2016-2019.csv']"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path_lemma = [data_path+\"lemma\\\\\"+f for f in os.listdir(data_path+\"lemma\")]\n",
    "\n",
    "data_lemma = dict()\n",
    "for i, time in enumerate(times):\n",
    "    print('-'*10, time)\n",
    "    data_lemma[time] = pd.read_csv(data_path_lemma[i], index_col=0)\n",
    "    \n",
    "data_path_lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# transform to node and edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate all nodes\n",
    "def generate_nodes(df, col):\n",
    "    afs = list(df[col])\n",
    "    afs_list = []\n",
    "    for af in afs:\n",
    "        if type(af)==str:\n",
    "            afs_list.append(af.split('; '))\n",
    "        else:\n",
    "            afs_list.append('')\n",
    "            # print(af)\n",
    "    all_afs = sorted(list(set(([name for names in afs_list for name in names]))))\n",
    "    return all_afs, afs_list\n",
    "\n",
    "def generate_all_nodes(data, col):\n",
    "    all_nodes = []\n",
    "    nodes = dict()\n",
    "    nodes_list = dict()\n",
    "    for i, time in enumerate(times):\n",
    "        df = data[time]\n",
    "        nodes[time], nodes_list[time] = generate_nodes(df, col)\n",
    "        all_nodes += nodes[time]\n",
    "        all_nodes = sorted(list(set(all_nodes)))\n",
    "\n",
    "        print(time, len(nodes[time]), len(all_nodes)) \n",
    "    \n",
    "    return nodes, nodes_list, all_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_df_nodes(nodes, all_nodes):\n",
    "    df_nodes = dict()\n",
    "    for i, time in enumerate(times):\n",
    "        \n",
    "        df_node = pd.DataFrame(columns=['id', 'label'])\n",
    "        node_id_list, node_label_list = [], []\n",
    "\n",
    "        for name in nodes[time]:\n",
    "            node_id_list.append(all_nodes.index(name)+1)\n",
    "            node_label_list.append(name)\n",
    "\n",
    "        df_node['id'] = node_id_list\n",
    "        df_node['label'] = node_label_list\n",
    "\n",
    "        df_nodes[time] = df_node  \n",
    "\n",
    "    return df_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_node_id(nodes_list, all_nodes):\n",
    "    nodes_id = dict()\n",
    "    \n",
    "    for i, time in enumerate(times):\n",
    "        \n",
    "        node_id_list = []\n",
    "        for li in nodes_list[time]:\n",
    "\n",
    "            id_li = []\n",
    "            for word in li:\n",
    "                word_id = all_nodes.index(word)+1\n",
    "                id_li.append(word_id)\n",
    "\n",
    "            node_id_list.append(id_li)\n",
    "            \n",
    "        nodes_id[time] = node_id_list\n",
    "    return nodes_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_nodes_id_pair_list(nodes_id):\n",
    "    nodes_id_pair_list = dict()\n",
    "    for i, time in enumerate(times):\n",
    "        all_pair_list = []\n",
    "        for cooccurr in nodes_id[time]:\n",
    "            pair_list = []\n",
    "            for pair in itertools.combinations(cooccurr, 2):\n",
    "                pair_list.append(pair)\n",
    "\n",
    "            all_pair_list.append(pair_list)\n",
    "        nodes_id_pair_list[time] = all_pair_list\n",
    "    return nodes_id_pair_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_df_edges(nodes_id_pair_list):    \n",
    "    df_edges = dict()\n",
    "    for i, time in enumerate(times):\n",
    "        all_pairs = []\n",
    "        for pairs in nodes_id_pair_list[time]:\n",
    "            all_pairs+=pairs\n",
    "\n",
    "        source_list, target_list = [], []\n",
    "        df_edge = pd.DataFrame(columns=['source', 'target', 'weight'])\n",
    "\n",
    "        for pair in all_pairs:\n",
    "            source_list.append(pair[0])\n",
    "            target_list.append(pair[1])\n",
    "\n",
    "        df_edge['source'] = source_list\n",
    "        df_edge['target'] = target_list\n",
    "        df_edge['weight'] = [1]*len(all_pairs)\n",
    "        df_edge = df_edge.sort_values(by=['source', 'target'])\n",
    "        df_edge.index = list(range(len(df_edge)))\n",
    "\n",
    "        df_edges[time] = df_edge\n",
    "\n",
    "    return df_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_weight(df_edges):\n",
    "    # x = pd.DataFrame(columns=['source', 'target', 'weight'])\n",
    "    # x['source'] = [1,1,1,2,3,3,4,5,5,5]\n",
    "    # x['target'] = [2,2,2,3,4,4,5,1,1,1]\n",
    "    # x['weight'] = [1,1,1,1,1,1,1,1,1,1]\n",
    "    # x = df_edges[0][['source', 'target', 'weight']]\n",
    "    \n",
    "    df_weighted_edges = dict()\n",
    "    for i, time in enumerate(times):\n",
    "        df = df_edges[time].copy()\n",
    "        x = df[['source', 'target', 'weight']]\n",
    "        x.loc[len(x.index)] = ['end', 'end', 1] # use end index to subtract the weight by index number in the end\n",
    "\n",
    "        y = x.drop_duplicates(subset=['source', 'target'])\n",
    "        y_index = list(y.index)\n",
    "        # print(y_index)\n",
    "        new_weight = []\n",
    "        for i, index in enumerate(y_index):\n",
    "            if i+1<len(y_index):\n",
    "                new_weight.append(y_index[i+1]-y_index[i])\n",
    "        y = y[y['source']!='end']\n",
    "        y['weight'] = new_weight\n",
    "        y = y.sort_values(by=['source', 'target'])\n",
    "        df_weighted_edges[time] = y\n",
    "        \n",
    "    return df_weighted_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(nodes, nodes_list, all_nodes):\n",
    "    df_nodes = generate_df_nodes(nodes, all_nodes)\n",
    "    nodes_id = generate_node_id(nodes_list, all_nodes)\n",
    "    nodes_id_pair_list = generate_nodes_id_pair_list(nodes_id)\n",
    "    df_edges = generate_df_edges(nodes_id_pair_list)\n",
    "    df_weighted_edges = generate_weight(df_edges)\n",
    "    \n",
    "    return df_nodes, df_weighted_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "co-occurrence\n",
      "-------------------- origin\n",
      "2010-2015 4351 4351\n",
      "2016-2016 1223 5113\n",
      "2016-2017 2349 5975\n",
      "2016-2018 3364 6816\n",
      "2016-2019 4172 7512\n",
      "\n",
      " -------------------- lemma\n",
      "2010-2015 4097 4097\n",
      "2016-2016 1190 4820\n",
      "2016-2017 2256 5618\n",
      "2016-2018 3208 6400\n",
      "2016-2019 3963 7042\n",
      "\n",
      " -------------------- stem\n",
      "2010-2015 4010 4010\n",
      "2016-2016 1178 4709\n",
      "2016-2017 2221 5483\n",
      "2016-2018 3148 6245\n",
      "2016-2019 3884 6867\n"
     ]
    }
   ],
   "source": [
    "if entity=='co-occurrence':\n",
    "    col = 'DE'\n",
    "    print('co-occurrence')\n",
    "    print('-'*20, 'origin')\n",
    "    nodes, nodes_list, all_nodes = generate_all_nodes(data, col)\n",
    "\n",
    "    print('\\n', '-'*20, 'lemma')\n",
    "    nodes_lemma, nodes_list_lemma, all_nodes_lemma = generate_all_nodes(data_lemma, col)\n",
    "\n",
    "    print('\\n', '-'*20, 'stem')\n",
    "    nodes_stem, nodes_list_stem, all_nodes_stem = generate_all_nodes(data_stem, col)\n",
    "elif entity=='co-author':\n",
    "    col = 'AF'\n",
    "    print(\"co-author\")\n",
    "    print('-'*20)\n",
    "    nodes, nodes_list, all_nodes = generate_all_nodes(data, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\USER\\\\Desktop\\\\linchengwei_linkpred\\\\data_transformation_output\\\\scientometrics\\\\co-occurrence\\\\lemma\\\\2010-2015',\n",
       " 'C:\\\\Users\\\\USER\\\\Desktop\\\\linchengwei_linkpred\\\\data_transformation_output\\\\scientometrics\\\\co-occurrence\\\\lemma\\\\2016-2016',\n",
       " 'C:\\\\Users\\\\USER\\\\Desktop\\\\linchengwei_linkpred\\\\data_transformation_output\\\\scientometrics\\\\co-occurrence\\\\lemma\\\\2016-2017',\n",
       " 'C:\\\\Users\\\\USER\\\\Desktop\\\\linchengwei_linkpred\\\\data_transformation_output\\\\scientometrics\\\\co-occurrence\\\\lemma\\\\2016-2018',\n",
       " 'C:\\\\Users\\\\USER\\\\Desktop\\\\linchengwei_linkpred\\\\data_transformation_output\\\\scientometrics\\\\co-occurrence\\\\lemma\\\\2016-2019']"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_output_dir(output_path):\n",
    "    for time in times:\n",
    "        try:\n",
    "            os.mkdir(output_path+time)\n",
    "        except FileExistsError:\n",
    "            print(\"File Existed!!\")\n",
    "            \n",
    "    output_path = [output_path+t for t in os.listdir(output_path) if t.startswith('20')]\n",
    "    return output_path\n",
    "\n",
    "output_path = path+\"\\\\data_transformation_output\\\\{}\\\\{}\\\\\".format(journal, entity)\n",
    "output_lemma_path = path+\"\\\\data_transformation_output\\\\{}\\\\{}\\\\lemma\\\\\".format(journal, entity)\n",
    "output_stem_path = path+\"\\\\data_transformation_output\\\\{}\\\\{}\\\\stem\\\\\".format(journal, entity)\n",
    "\n",
    "output_lemma_path = generate_output_dir(output_lemma_path)\n",
    "output_lemma_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\USER\\\\Desktop\\\\linchengwei_linkpred\\\\data_transformation_output\\\\scientometrics\\\\co-occurrence\\\\stem\\\\2010-2015',\n",
       " 'C:\\\\Users\\\\USER\\\\Desktop\\\\linchengwei_linkpred\\\\data_transformation_output\\\\scientometrics\\\\co-occurrence\\\\stem\\\\2016-2016',\n",
       " 'C:\\\\Users\\\\USER\\\\Desktop\\\\linchengwei_linkpred\\\\data_transformation_output\\\\scientometrics\\\\co-occurrence\\\\stem\\\\2016-2017',\n",
       " 'C:\\\\Users\\\\USER\\\\Desktop\\\\linchengwei_linkpred\\\\data_transformation_output\\\\scientometrics\\\\co-occurrence\\\\stem\\\\2016-2018',\n",
       " 'C:\\\\Users\\\\USER\\\\Desktop\\\\linchengwei_linkpred\\\\data_transformation_output\\\\scientometrics\\\\co-occurrence\\\\stem\\\\2016-2019']"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_stem_path = generate_output_dir(output_stem_path)\n",
    "output_stem_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_files(nodes, nodes_list, all_nodes, output_path):\n",
    "    df_nodes, df_weighted_edges = main(nodes, nodes_list, all_nodes)\n",
    "\n",
    "    for i, time in enumerate(times):\n",
    "        df_nodes[time].to_csv(output_path[i]+'\\\\node.csv')\n",
    "        df_weighted_edges[time].to_csv(output_path[i]+'\\\\edge.csv')\n",
    "        \n",
    "save_files(nodes_stem, nodes_list_stem, all_nodes_stem, output_stem_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
